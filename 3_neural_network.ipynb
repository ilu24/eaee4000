{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b241f4c",
   "metadata": {},
   "source": [
    "# accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5a8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls\n",
    "#!pip install pandas\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0361512-4af0-4a7c-a53d-9414fe8f477e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/alexanderfeil/Desktop/studies/2 Master/2 Fall Term/machine learning/project/FPA-FOD_reduced.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/alexanderfeil/Desktop/studies/2 Master/2 Fall Term/machine learning/project/FPA-FOD_reduced.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#drops the items where columns are unnamed\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/alexanderfeil/Desktop/studies/2 Master/2 Fall Term/machine learning/project/FPA-FOD_reduced.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/alexanderfeil/Desktop/studies/2 Master/2 Fall Term/machine learning/project/FPA-FOD_reduced.csv\", low_memory = False)\n",
    "df = df.drop('Unnamed: 0', axis = 'columns') #drops the items where columns are unnamed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428ac48-b03f-452c-9cf9-20720ae28910",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29dd24fe-ce06-4869-9fa4-468ef138c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_mask = df['NWCG_GENERAL_CAUSE'] == \"Missing data/not specified/undetermined\"\n",
    "\n",
    "df_known = df[~unknown_mask].copy()\n",
    "df_unknown = df[unknown_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f010ee4-63fb-4ed7-a147-ed5169a8bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_known.drop(columns=['NWCG_GENERAL_CAUSE'])\n",
    "y = df_known['NWCG_GENERAL_CAUSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78900e44-d173-4c89-8c58-9b45003a83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(X.median(numeric_only=True)) #replace NANs\n",
    "\n",
    "for col in X.columns: #convert numeric values to floats, if possible\n",
    "    if X[col].dtype == 'object':\n",
    "        # try converting to float — if it works, keep it numeric\n",
    "        try:\n",
    "            X[col] = X[col].astype(float)\n",
    "        except ValueError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2adb3352-654d-4cb7-9efd-b89977cf8213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y) #encode the textual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37fdfd01-1723-41a5-9140-7446427a1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features = ['pr', 'tmmn', 'tmmx', 'rmin', 'rmax', 'sph', 'srad', 'etr', 'vpd',\n",
    "                    'bi', 'erc', 'fm100', 'fm1000', 'EVC', 'EVT', 'EVH',\n",
    "                    'Elevation', 'Slope', 'Aspect', 'TRI', 'TPI', 'Aridity_index',\n",
    "                    'Population', 'GDP', 'LATITUDE', 'LONGITUDE', 'FIRE_YEAR','DISCOVERY_DOY', 'DISCOVERY_TIME']\n",
    "\n",
    "categorical_features = ['STATE', 'COUNTY']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ded3c-daca-4b6e-9a7d-8c469480a4af",
   "metadata": {},
   "source": [
    "# Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ce1d53-ec42-4d3d-980f-1c19c79c0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/1xc4qsqs6wv7ng86fqksqfrr0000gn/T/ipykernel_44296/1772595633.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df2.groupby('cause', group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90186, 32) (22547, 32)\n",
      "cause\n",
      "11    8000\n",
      "6     8000\n",
      "10    8000\n",
      "8     8000\n",
      "0     8000\n",
      "7     8000\n",
      "2     8000\n",
      "4     8000\n",
      "1     8000\n",
      "5     8000\n",
      "9     8000\n",
      "3     2186\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine X and y into one DataFrame temporarily\n",
    "df2 = X.copy()\n",
    "df2['cause'] = y_encoded\n",
    "\n",
    "df_balanced = (\n",
    "    df2.groupby('cause', group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=min(len(x), 10000), random_state=42))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Split back into X and y\n",
    "X_balanced = df_balanced.drop(columns=['cause'])\n",
    "y_balanced = df_balanced['cause']\n",
    "\n",
    "# Now do your train-test split as usual\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(X_train_df.shape, X_test_df.shape)\n",
    "print(y_train_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4444d9-cf32-4bcc-820f-2a300b61332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cause_id                                 cause_name  count\n",
      "       11                Firearms and explosives use   8000\n",
      "        6        Railroad operations and maintenance   8000\n",
      "       10                               Other causes   8000\n",
      "        8                                    Natural   8000\n",
      "        0                    Debris and open burning   8000\n",
      "        7                    Recreation and ceremony   8000\n",
      "        2                  Equipment and vehicle use   8000\n",
      "        4                         Arson/incendiarism   8000\n",
      "        1                  Misuse of fire by a minor   8000\n",
      "        5 Power generation/transmission/distribution   8000\n",
      "        9                                    Smoking   8000\n",
      "        3                                  Fireworks   2186\n"
     ]
    }
   ],
   "source": [
    "cause_names = [\n",
    "    \"Debris and open burning\",\n",
    "    \"Misuse of fire by a minor\",\n",
    "    \"Equipment and vehicle use\",\n",
    "    \"Fireworks\",\n",
    "    \"Arson/incendiarism\",\n",
    "    \"Power generation/transmission/distribution\",\n",
    "    \"Railroad operations and maintenance\",\n",
    "    \"Recreation and ceremony\",\n",
    "    \"Natural\",\n",
    "    \"Smoking\",\n",
    "    \"Other causes\",\n",
    "    \"Firearms and explosives use\"\n",
    "]\n",
    "\n",
    "# Convert y_train value counts into a DataFrame for easier mapping\n",
    "counts = y_train_df.value_counts().reset_index()\n",
    "counts.columns = ['cause_id', 'count']\n",
    "\n",
    "# Map encoded ID to name\n",
    "counts['cause_name'] = counts['cause_id'].map(lambda i: cause_names[i])\n",
    "\n",
    "print(counts[['cause_id', 'cause_name', 'count']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff0cdac0-258c-4f98-971c-d45d052224a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr</th>\n",
       "      <th>tmmn</th>\n",
       "      <th>tmmx</th>\n",
       "      <th>rmin</th>\n",
       "      <th>rmax</th>\n",
       "      <th>sph</th>\n",
       "      <th>srad</th>\n",
       "      <th>etr</th>\n",
       "      <th>vpd</th>\n",
       "      <th>bi</th>\n",
       "      <th>...</th>\n",
       "      <th>Aridity_index</th>\n",
       "      <th>Population</th>\n",
       "      <th>GDP</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>289.299988</td>\n",
       "      <td>305.600006</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>78.099998</td>\n",
       "      <td>0.01047</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.68</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>29129.756</td>\n",
       "      <td>33.172944</td>\n",
       "      <td>-89.923202</td>\n",
       "      <td>MS</td>\n",
       "      <td>Holmes</td>\n",
       "      <td>1995</td>\n",
       "      <td>250</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104451</th>\n",
       "      <td>0.0</td>\n",
       "      <td>287.600006</td>\n",
       "      <td>303.399994</td>\n",
       "      <td>32.100002</td>\n",
       "      <td>95.700005</td>\n",
       "      <td>0.01010</td>\n",
       "      <td>256.300018</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.35</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>67082.420</td>\n",
       "      <td>39.770000</td>\n",
       "      <td>-74.679000</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>2015</td>\n",
       "      <td>249</td>\n",
       "      <td>2300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57302</th>\n",
       "      <td>3.6</td>\n",
       "      <td>295.299988</td>\n",
       "      <td>308.600006</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.01639</td>\n",
       "      <td>357.899994</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>50895.010</td>\n",
       "      <td>29.821390</td>\n",
       "      <td>-81.520000</td>\n",
       "      <td>FL</td>\n",
       "      <td>St. Johns</td>\n",
       "      <td>2006</td>\n",
       "      <td>217</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100210</th>\n",
       "      <td>0.0</td>\n",
       "      <td>273.200012</td>\n",
       "      <td>286.600006</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>63.200001</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>216.400009</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.67</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>45981.348</td>\n",
       "      <td>47.647800</td>\n",
       "      <td>-114.345600</td>\n",
       "      <td>MT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020</td>\n",
       "      <td>82</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74912</th>\n",
       "      <td>0.0</td>\n",
       "      <td>274.600006</td>\n",
       "      <td>283.399994</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>66.200005</td>\n",
       "      <td>0.00310</td>\n",
       "      <td>258.899994</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.49</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>42784.630</td>\n",
       "      <td>46.262150</td>\n",
       "      <td>-89.783430</td>\n",
       "      <td>WI</td>\n",
       "      <td>Vilas</td>\n",
       "      <td>2010</td>\n",
       "      <td>106</td>\n",
       "      <td>1342.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pr        tmmn        tmmx       rmin        rmax      sph  \\\n",
       "112173  0.0  289.299988  305.600006  34.500000   78.099998  0.01047   \n",
       "104451  0.0  287.600006  303.399994  32.100002   95.700005  0.01010   \n",
       "57302   3.6  295.299988  308.600006  38.400002  100.000000  0.01639   \n",
       "100210  0.0  273.200012  286.600006  24.100000   63.200001  0.00276   \n",
       "74912   0.0  274.600006  283.399994  36.400002   66.200005  0.00310   \n",
       "\n",
       "              srad  etr   vpd    bi  ...  Aridity_index  Population  \\\n",
       "112173  279.000000  5.9  1.68  22.0  ...           0.75      0.0380   \n",
       "104451  256.300018  6.7  1.35  32.0  ...           0.73      0.0000   \n",
       "57302   357.899994  8.2  1.56   0.0  ...           0.42      0.0000   \n",
       "100210  216.400009  3.8  0.67  33.0  ...           0.26      0.0042   \n",
       "74912   258.899994  5.3  0.49  59.0  ...           0.78      0.0150   \n",
       "\n",
       "              GDP   LATITUDE   LONGITUDE  STATE      COUNTY  FIRE_YEAR  \\\n",
       "112173  29129.756  33.172944  -89.923202     MS      Holmes       1995   \n",
       "104451  67082.420  39.770000  -74.679000     NJ  Burlington       2015   \n",
       "57302   50895.010  29.821390  -81.520000     FL   St. Johns       2006   \n",
       "100210  45981.348  47.647800 -114.345600     MT         NaN       2020   \n",
       "74912   42784.630  46.262150  -89.783430     WI       Vilas       2010   \n",
       "\n",
       "        DISCOVERY_DOY  DISCOVERY_TIME  \n",
       "112173            250          1400.0  \n",
       "104451            249          2300.0  \n",
       "57302             217          1600.0  \n",
       "100210             82          2200.0  \n",
       "74912             106          1342.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b88c188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112173    11\n",
       "104451    11\n",
       "57302      6\n",
       "100210    10\n",
       "74912      8\n",
       "Name: cause, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c54c0-5565-4a32-aff4-3df005c5a7b6",
   "metadata": {},
   "source": [
    "## you can start from here really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4da3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn tables into CSV files\n",
    "\n",
    "# X_train_df.to_csv(\"X_train_df.csv\", index=False)\n",
    "# y_train_df.to_csv(\"y_train_df.csv\", index=False)\n",
    "# X_test_df.to_csv(\"X_test_df.csv\", index=False)\n",
    "# y_test_df.to_csv(\"y_test_df.csv\", index=False)\n",
    "\n",
    "# load CSV files\n",
    "X_train_df = pd.read_csv(\"datasets/X_train_df.csv\")\n",
    "y_train_df = pd.read_csv(\"datasets/y_train_df.csv\")\n",
    "X_test_df = pd.read_csv(\"datasets/X_test_df.csv\")\n",
    "y_test_df = pd.read_csv(\"datasets/y_test_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3454694c",
   "metadata": {},
   "source": [
    "### Standardizazion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2786f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Standardization\n",
    "X_train_df = X_train_df.select_dtypes(include=['number'])\n",
    "y_train_df = y_train_df.select_dtypes(include=['number'])\n",
    "X_test_df = X_test_df.select_dtypes(include=['number'])\n",
    "X_train_df = X_train_df.select_dtypes(include=['number'])\n",
    "\n",
    "mean, std = X_train_df.mean(), X_train_df.std()\n",
    "\n",
    "X_train_df   = (X_train_df - mean)/std\n",
    "X_test_df    = (X_test_df - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "823a91a3-c083-4201-b4ab-6ba7512ec331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train_df.to_numpy()\n",
    "y_train = y_train_df.to_numpy()\n",
    "\n",
    "X_test = X_test_df.to_numpy()\n",
    "\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), \\\n",
    "                                               torch.FloatTensor(y_train))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test))\n",
    "\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0869234-79af-4806-949c-b9645fd418ed",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8305ba6b-ebe3-4e99-a578-fb6a8a3e334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import * \n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 400\n",
    "plt.rcParams['font.size'] = 13\n",
    "plt.rcParams[\"legend.frameon\"] = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb0a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "n_neuron       = 64\n",
    "activation     = 'ReLU' #'ReLU' #'Linear', 'Tanh'\n",
    "num_epochs     = 50\n",
    "learning_rate  = 0.001\n",
    "minibatch_size = 64\n",
    "N_layers       = 2 # number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b0f5e93-8126-42dc-aef5-17483875befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for batched training\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=minibatch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=minibatch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e1ad290-150b-4e58-94cc-a3f170eb2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(X_train.shape[1], n_neuron))\n",
    "layers.append(getattr(nn, activation)())\n",
    "\n",
    "for n in range(N_layers-1):\n",
    "   layers.append(nn.Linear(n_neuron, n_neuron))\n",
    "   layers.append(getattr(nn, activation)())\n",
    "\n",
    "layers.append(nn.Linear(n_neuron, y_train.shape[1]))\n",
    "\n",
    "model = nn.Sequential(*layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a4610b8-5b0c-4199-b782-2184bae2d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=29, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "101fc40d-03f8-44b7-80d4-42e2dd776e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=29, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af83b3-c4ce-4d5b-865a-72460eee2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 12.7531, Val Loss: 11.6906\n",
      "Epoch 2/50, Train Loss: 11.4243, Val Loss: 11.4545\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 20\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   # training\n",
    "   model.train()\n",
    "   train_loss = 0.0\n",
    "   for batch_X, batch_y in train_loader:\n",
    "       batch_X = batch_X.to(device)\n",
    "       batch_y = batch_y.to(device)\n",
    "       # forward pass\n",
    "       optimizer.zero_grad()\n",
    "       outputs = model(batch_X)\n",
    "       loss = criterion(outputs, batch_y)\n",
    "       # backward pass\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "   model.eval()\n",
    "   val_loss = 0.0\n",
    "   with torch.no_grad():\n",
    "       for batch_X, batch_y in val_loader:\n",
    "           batch_X = batch_X.to(device)\n",
    "           batch_y = batch_y.to(device)\n",
    "           \n",
    "           outputs = model(batch_X)\n",
    "           loss = criterion(outputs, batch_y)\n",
    "           val_loss += loss.item()\n",
    "   \n",
    "   train_loss /= len(train_loader)\n",
    "   val_loss /= len(val_loader)\n",
    "\n",
    "   train_losses.append(train_loss)\n",
    "   val_losses.append(val_loss)\n",
    "   \n",
    "   print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "   \n",
    "   if val_loss < best_val_loss:\n",
    "       best_val_loss = val_loss\n",
    "       patience_counter = 0\n",
    "   else:\n",
    "       patience_counter += 1\n",
    "       if patience_counter >= patience:\n",
    "           print(f'Early stopping at epoch {epoch+1}')\n",
    "           break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a8775-19b9-449b-a7db-e5c8d57d44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5fcbb0-0b65-4602-8bf9-f4487825d9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
